{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data preparation\n",
    "* Read the downloaded input dataset."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "df = pd.read_csv(\"./data/songdata.csv\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  artist                   song                                        link  \\\n",
       "0   ABBA  Ahe's My Kind Of Girl  /a/abba/ahes+my+kind+of+girl_20598417.html   \n",
       "1   ABBA       Andante, Andante       /a/abba/andante+andante_20002708.html   \n",
       "2   ABBA         As Good As New        /a/abba/as+good+as+new_20003033.html   \n",
       "3   ABBA                   Bang                  /a/abba/bang_20598415.html   \n",
       "4   ABBA       Bang-A-Boomerang      /a/abba/bang+a+boomerang_20002668.html   \n",
       "\n",
       "                                                text  \n",
       "0  Look at her face, it's a wonderful face  \\nAnd...  \n",
       "1  Take it easy with me, please  \\nTouch me gentl...  \n",
       "2  I'll never know why I had to go  \\nWhy I had t...  \n",
       "3  Making somebody happy is a question of give an...  \n",
       "4  Making somebody happy is a question of give an...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song</th>\n",
       "      <th>link</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Ahe's My Kind Of Girl</td>\n",
       "      <td>/a/abba/ahes+my+kind+of+girl_20598417.html</td>\n",
       "      <td>Look at her face, it's a wonderful face  \\nAnd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Andante, Andante</td>\n",
       "      <td>/a/abba/andante+andante_20002708.html</td>\n",
       "      <td>Take it easy with me, please  \\nTouch me gentl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>As Good As New</td>\n",
       "      <td>/a/abba/as+good+as+new_20003033.html</td>\n",
       "      <td>I'll never know why I had to go  \\nWhy I had t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang</td>\n",
       "      <td>/a/abba/bang_20598415.html</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang-A-Boomerang</td>\n",
       "      <td>/a/abba/bang+a+boomerang_20002668.html</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "* Our dataset consists of about $57,650$ song lyrics."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "df.shape[0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "57650"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "* We have song lyrics of about $643$ artists."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "len(df['artist'].unique())"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "643"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "* The number of songs from each artist is shown as follows."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "df['artist'].value_counts()[:10]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Donna Summer        191\n",
       "Gordon Lightfoot    189\n",
       "George Strait       188\n",
       "Bob Dylan           188\n",
       "Reba Mcentire       187\n",
       "Alabama             187\n",
       "Cher                187\n",
       "Loretta Lynn        187\n",
       "Chaka Khan          186\n",
       "Dean Martin         186\n",
       "Name: artist, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "* On average, we have about $89$ songs of each artist."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "df['artist'].value_counts().values.mean()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "89.65785381026438"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "* We have song lyrics in the column text, so we combine all the rows of that column and save it as a text in a variable called `data`, as follows."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "data = ', '.join(df['text'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "* Let's see few lines of song."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "data[:369]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"Look at her face, it's a wonderful face  \\nAnd it means something special to me  \\nLook at the way that she smiles when she sees me  \\nHow lucky can one fellow be?  \\n  \\nShe's just my kind of girl, she makes me feel fine  \\nWho could ever believe that she could be mine?  \\nShe's just my kind of girl, without her I'm blue  \\nAnd if she ever leaves me what could I do, what co\""
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "* Since we are building a char-level RNN, we will store all the unique characters in our dataset into a variable called `chars`. This is basically our vocabulary."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "chars = sorted(list(set(data)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "* Store the vocabulary size in a variable called `vocab_size`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "vocab_size = len(chars)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "* Since the neural network only accept the input in numbers, we need to convert all the characters in the vocabulary to a number.\n",
    "* We map all the characters in the vocabulary to their corresponding index that forms a unique. We define a `char_to_ix` dictionary, which has a mapping of all the characters to the index. To get the index by a character, we also define the `ix_to_char` dictionary, which has mapping of all the indices to their respective characters."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "char_to_ix = {ch:i for i, ch in enumerate(chars)}\n",
    "ix_to_char = {i:ch for i, ch in enumerate(chars)}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "* As you can see in the following code snippet, the character's is mapped to an index 68 in the `char_to_ix` dictionary."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "char_to_ix['s']"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "ix_to_char[68]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'s'"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "* Once we obtain the character to integer mapping, we use one-hot encoding to represent the input and output in vector form. A one-hot encoded vector is basically a vector full of $0$s, except for a $1$ at a position corresponding to a character index.\n",
    "* For example, let's suppose that the `vocabSize` is $7$, and the character `z` is in the fourth position in the vocabulary. Then, the one-hot encoded representation for the charactrer `z` can be represented as follows."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "vocabSize = 7\n",
    "char_index = 4\n",
    "\n",
    "np.eye(vocabSize)[char_index]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 1., 0., 0.])"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "* As you can see, we have a $1$ at the corresponding index of the character, and the rest of the values are $0$s. This is how we convert each character into a one-hot encoded vector.\n",
    "* In the following code, we define a function called `one_hot_encoder`, which will return the one-hot encoded vectors, given an index of the character."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "def one_hot_encoder(index):\n",
    "    return np.eye(vocab_size)[index]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Defining the Network Parameters\n",
    "* We need to define all the network parameters."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "'''Define the number of units in the hidden layer'''\n",
    "hidden_size = 100\n",
    "\n",
    "'''Define the length of the input and output sequence'''\n",
    "seq_length = 25\n",
    "\n",
    "'''Define the learning rate for gradient descent is as follows'''\n",
    "learning_rate = 1e-1\n",
    "\n",
    "'''Set the seed value'''\n",
    "seed_value = 42\n",
    "tf.set_random_seed(seed_value)\n",
    "random.seed(seed_value)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Defining placeholders\n",
    "* Now, we will define the TensorFlow placeholders. The placeholders for the input and outut are as the follows."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "inputs = tf.placeholder(shape=[None, vocab_size], dtype=tf.float32, name='inputs')\n",
    "targets = tf.placeholder(shape=[None, vocab_size], dtype=tf.float32, name='targets')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "* Define the placeholder for the initial hidden state."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "init_state = tf.placeholder(shape=[1, hidden_size], dtype=tf.float32, name='state')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "* Define an initializer for initializing the weights of the RNN."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "initializer = tf.random_normal_initializer(stddev=0.1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Defining forward propagation\n",
    "* Let's define the forward propagation involved in the RNN, which is mathematically given as follows.\n",
    "  $$\\begin{aligned}\n",
    "    &\\mathbf{h}_t = \\mathrm{tanh}(\\mathbf{Ux}_t + \\mathbf{Wh}_{t - 1} + \\mathbf{bh}) \\\\\n",
    "    &\\widehat{\\mathbf{y}} = \\mathrm{softmax}(\\mathbf{Vh}_t + \\mathbf{bv})\n",
    "  \\end{aligned}$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "with tf.variable_scope('RNN') as scope:\n",
    "    h_t = init_state\n",
    "    y_hat = []\n",
    "    \n",
    "    for t, x_t in enumerate(tf.split(inputs, seq_length, axis=0)):\n",
    "        if t > 0:\n",
    "            scope.reuse_variables()\n",
    "            \n",
    "        '''Input to hidden layer weights'''\n",
    "        U = tf.get_variable('U', [vocab_size, hidden_size], initializer=initializer)\n",
    "        \n",
    "        '''Hidden to hidden layer weights'''\n",
    "        W = tf.get_variable('W', [hidden_size, hidden_size], initializer=initializer)\n",
    "        \n",
    "        '''Hidden to output layer weights'''\n",
    "        V = tf.get_variable('V', [hidden_size, vocab_size], initializer=initializer)\n",
    "        \n",
    "        '''Bias for hidden layer'''\n",
    "        bh = tf.get_variable('bh', [hidden_size], initializer=initializer)\n",
    "        \n",
    "        '''Bias for output layer'''\n",
    "        by = tf.get_variable('by', [vocab_size], initializer=initializer)\n",
    "        \n",
    "        h_t = tf.tanh(tf.matmul(x_t, U) + tf.matmul(h_t, W) + bh)\n",
    "        \n",
    "        y_hat_t = tf.matmul(h_t, V) + by\n",
    "        \n",
    "        y_hat.append(y_hat_t)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "* Apply softmax on the output and get the probabilities."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "output_softmax = tf.nn.softmax(y_hat[-1])\n",
    "outputs = tf.concat(y_hat, axis=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "* Computer the cross-entropy loss."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(labels=targets, logits=outputs))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "* Store the final hidden state of the RNN in `hprev`. We use this hidden state for making predictions."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "hprev = h_t"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Defining Backpropagation Through time\n",
    "* Now, we will perform the BPTT, with Adam as out optimizer. We will also perform gradient clipping to avoid the exploding gradients problem.\n",
    "* Initialize the Adam optimizer."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "minimizer = tf.train.AdamOptimizer()  "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "* Computer the gradients of the loss with the Adam optimizer."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "gradients = minimizer.compute_gradients(loss)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "* Set the threshold for the gradient clipping."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "threshold = tf.constant(5., name='grad_clipping')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "* Clip the gradients which exceeds the `threshold` and bring it to the range."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "clipped_gradients = []\n",
    "\n",
    "for grad, var in gradients:\n",
    "    clipped_grad = tf.clip_by_value(grad, -threshold, threshold)\n",
    "    clipped_gradients.append((clipped_grad, var))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "* Update the gradients with the clipped gradients."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "updated_gradients = minimizer.apply_gradients(clipped_gradients)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Start generating songs\n",
    "* Start the TensorFlow session and initialize all the variables."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Step by Step explanation\n",
    "##### Abstract: \n",
    "* First let us understand how RNN is generating song lyrics step by step, the complete code is given at the end of the notebook.\n",
    "\n",
    "<hr>\n",
    "\n",
    "* Now, we will look at how to generate the song lyrics using an RNN. What should the input and output to the RNN be? How does it learn? What is the training data? Let's see the explanation, along with the code, step by step.\n",
    "* We know that in RNNs, the output predicted at a time step $t$ will be sent as the input to the next time step. We need to feed the predicted character from the previous time step as input. So, we prepare out dataset in the same way.\n",
    "* For instance, look at the following table. Let's suppose that each row is a different time step, one time step $t = 0$, the RNN predicted a new chatacter `g` as the output. This will be sent as the input to the next time step $t = 1$.\n",
    "* However, if you notice the input in the time step $t = 1$, we removed the first character from the input `o` and added the newly predicted character `g` at the end of our sequence. Why are we removing the first character from the input? Because we need to maintain the sequence length.\n",
    "* Let's suppose that our sequence length is $8$, adding a newly predicted character to our sequence increases the sequence length to $9$. To avoid this, we remove the first character from the input, while adding a newly predicted character from the previous time step.\n",
    "* Similarity, in the output data, we also remove the first character on each time step. Because once it predicts the new character, the sequence length increases. To void this, we remove the first character from the output on each time step, as shown in the following table.<br>\n",
    "  ![](./images/04.00.png)\n",
    "* Now we will look at how we can prepare out input and output sequence similar to the preceding table.\n",
    "  * Define a variable called `pointer`, which points to the character in our dataset. We will set out pointer to $0$, which means it points to the first character.\n",
    "    ```python\n",
    "    pointer = 0\n",
    "    ```\n",
    "  * Define the input data.\n",
    "    ```python\n",
    "    input_sentence = data[pointer: pointer + seq_length]\n",
    "    ```\n",
    "  * What does this mean? With the pointer and the sequence length, we splice the data. Consider that the `seq_length` is $25$ an2 the pointer is $0$. It will return the first $25$ character as input. So `data[pointer: pointer + seq_length]` returns the following output:\n",
    "    > \"Look at her face, it's a \"\n",
    "\n",
    "  * Define the output, as follows:\n",
    "    ```python\n",
    "    output_sentence = data[pointer + 1: pointer + seq_length + 1]\n",
    "    ```\n",
    "\n",
    "  * We slice the output data with one character ahead moved from input data. So:\n",
    "    ```python\n",
    "    data[pointer + 1: pointer + seq_length + 1]\n",
    "    ```\n",
    "    returns the following:\n",
    "      > \"ook at her face, it's a w\"\n",
    "\n",
    "  * As you can see, we added the next character in the sentence and removed the first character. So, on every iteration, we increment the pointer and traverse the entire dataset. This is how we obtain the input and output sentence sentence for training the RNN.\n",
    "  * As you have learned, an RNN only accepts numbers as input. Thus, once we sliced the input and output sequence, we get the indices of the respective characters, using the `char_to_ix` dictionary that we defined.\n",
    "      ```python\n",
    "      input_indices = [char_to_ix[ch] for ch in input_sentence]\n",
    "      target_indices = [char_to_ix[ch] for ch in output_sentence]\n",
    "      ```\n",
    "  * Convert the indices into one-hot encoded vectors by using the `one_hot_encoder` function we defined previously.\n",
    "      ```python\n",
    "      input_vector = one_hot_encoder(input_indices)\n",
    "      target_vector = one_hot_encoder(target_indices)\n",
    "      ```\n",
    "\n",
    "  * This `input_vector` and `target_vector` become the input and output for training the RNN. Let's start training.\n",
    "  * The `hprev_val` variable stores the last hidden state of out trained RNN model. We use this for making predictions, and we store the loss in `loss_val`.\n",
    "      ```python\n",
    "      hprev_val, loss_val, _ = sess.run([hprev, loss, updated_gradients],\n",
    "                                      feed_dict={\n",
    "                                          inputs: input_vector,\n",
    "                                          targets: target_vector,\n",
    "                                          init_state: hprev_val\n",
    "                                      })\n",
    "      ```\n",
    "  * We train the model for $n$ iterations. After training, we start making predictions. Now, we will look at how to make predictions and generate song lyrics using our trained RNN. Set the `sample_length`, that is the length of the sentence (song) we want to generate.\n",
    "      ```python\n",
    "      sample_length = 500\n",
    "      ```\n",
    "  * Randomly select the starting index of the input sequence.\n",
    "      ```python\n",
    "      random_index = random.randint(0, len(data) - seq_length)\n",
    "      ```\n",
    "  * Select the input sentence with the randomly selected index.\n",
    "      ```python\n",
    "      sample_input_sent = data[random_index: random_index + seq_length]\n",
    "      ```\n",
    "  \n",
    "  * As we know, we need to feed the input as numbers. Convert the selected input sentence to indices:\n",
    "      ```python\n",
    "      sample_input_indices = [char_to_ix[ch] for ch in sample_input_sent]\n",
    "      ```\n",
    "\n",
    "  * Remember, we stored the last hidden state of the RNN in `hprev_val`. We used that for making predictions. Now, we will create a new variable called `sample_prev_state_val` by copying values from `hprev_val`.\n",
    "  * The `sample_prev_state_val` variable is used as an initial hidden state for making predictions:\n",
    "      ```python\n",
    "      sample_prev_state_val = np.copy(hprev_val)\n",
    "      ```\n",
    "  * Initialize the list for storing the predicted output indices:\n",
    "      ```python\n",
    "      predicted_indices = []\n",
    "      ```\n",
    "\n",
    "  * Now, for t in range of `sample_length`, we perform the following and generate the song for the defined `sample_length`. Convert the `sampled_input_indices` to the one-hot encoded vectors:\n",
    "    ```python\n",
    "    sample_input_vector = one_hot_encoder(sample_input_indices)\n",
    "    ```\n",
    "  \n",
    "  * Feed the `sample_input_vector`, and also the hidden state `sample_prev_state_val`, as the initial hidden state to the RNN, and get the predictions. We store the output probability distribution in `probs_dist`:\n",
    "    ```python\n",
    "    probs_dist, sample_prev_state_val = sess.run([output_softmax, hprev], \n",
    "                                                  feed_dict={\n",
    "                                                      inputs: sample_input_vector,\n",
    "                                                      init_state: sample_prev_state_val\n",
    "                                                  })\n",
    "    ```\n",
    "  * Randomly select the index of the next character with the probability distribution generated by the RNN:\n",
    "    ```python\n",
    "    ix = np.random.choice(range(vocab_size), p=probs_dist.ravel())\n",
    "    ```\n",
    "  * Add this newly predicted index `ix` to the `sample_input_indices`, and also remove the first index from `sample_input_indices` to maintain the sequence length. This will form the input for the next time step:  \n",
    "    ```python\n",
    "    sample_input_indices = sample_input_indices[1:] + [ix]\n",
    "    ```\n",
    "  \n",
    "  * Store all the predicted chars indices in the `predicted_indices` list:\n",
    "    ```python\n",
    "    predicted_indices.append(ix)\n",
    "    ```\n",
    "  \n",
    "  * Convert all the `predicted_indices` to their characters:\n",
    "    ```python\n",
    "    predicted_chars = [ix_to_char[ix] for ix in predicted_indices]\n",
    "    ```\n",
    "  \n",
    "  * Combine all the `predicted_chars` and save it as text:\n",
    "    ```python\n",
    "    text = ''.join(predicted_chars)\n",
    "    ```\n",
    "\n",
    "  * Print the predicted text on every 50,000th iteration:\n",
    "    ```python\n",
    "    print('\\n')\n",
    "    print(f\"After {iteration} iterations:\")\n",
    "    print(f\"\\n{text}\\n\")\n",
    "    print('-'*115)\n",
    "    ```\n",
    "\n",
    "  * Increment the pointer and iteration:\n",
    "    ```python\n",
    "    pointer += seq_length\n",
    "    iteration += 1\n",
    "    ```\n",
    "\n",
    "# Complete code block for generating songs"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "pointer = 0\n",
    "iteration = 0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "while True:\n",
    "    if pointer + seq_length + 1 >= len(data) or iteration == 0:\n",
    "        hprev_val = np.zeros([1, hidden_size])\n",
    "        pointer = 0\n",
    "        \n",
    "    '''Select input sentence'''\n",
    "    input_sentence = data[pointer: pointer + seq_length]\n",
    "    \n",
    "    '''Select output sentence'''\n",
    "    output_sentence = data[pointer + 1: pointer + seq_length + 1]\n",
    "    \n",
    "    '''Get the indices of input and output sentence'''\n",
    "    input_indices = [char_to_ix[ch] for ch in input_sentence]\n",
    "    target_indices = [char_to_ix[ch] for ch in output_sentence]\n",
    "    \n",
    "    '''Convert the input and output sentence to a one-hot encoded vectors with the help\n",
    "    of their indices'''\n",
    "    input_vector = one_hot_encoder(input_indices)\n",
    "    target_vector = one_hot_encoder(target_indices)\n",
    "    \n",
    "    '''Training the network and get the final hidden state'''\n",
    "    hprev_val, loss_val, _ = sess.run([hprev, loss, updated_gradients],\n",
    "                                      feed_dict={\n",
    "                                          inputs: input_vector,\n",
    "                                          targets: target_vector,\n",
    "                                          init_state: hprev_val\n",
    "                                      })\n",
    "    \n",
    "    '''Make predictions on every 500th iteration'''\n",
    "    if iteration % 500 == 0:\n",
    "        '''Length of characters we want to predict'''\n",
    "        sample_length = 500\n",
    "        \n",
    "        '''Random select index'''\n",
    "        random_index = random.randint(0, len(data) - seq_length)\n",
    "        \n",
    "        '''Sample the input sentence with the randomly selected index'''\n",
    "        sample_input_sent = data[random_index: random_index + seq_length]\n",
    "        \n",
    "        '''Get the indices of the sampled input sentence'''\n",
    "        sample_input_indices = [char_to_ix[ch] for ch in sample_input_sent]\n",
    "        \n",
    "        '''Store the final hidden state in sample_prev_state_val'''\n",
    "        sample_prev_state_val = np.copy(hprev_val)\n",
    "        \n",
    "        '''For storing the indices of predicted characters'''\n",
    "        predicted_indices = []\n",
    "        \n",
    "        for i in range(sample_length):\n",
    "            '''Convert the sampled input sentence into one-hot encoded vector using the indices'''\n",
    "            sample_input_vector = one_hot_encoder(sample_input_indices)\n",
    "            \n",
    "            '''Compute the probability of all the words in the vocabulary to the next character'''\n",
    "            probs_dist, sample_prev_state_val = sess.run([output_softmax, hprev], \n",
    "                                                         feed_dict={\n",
    "                                                             inputs: sample_input_vector,\n",
    "                                                             init_state: sample_prev_state_val\n",
    "                                                         })\n",
    "            \n",
    "            '''We randomly select the index with the probability distribution generated by the model'''\n",
    "            ix = np.random.choice(range(vocab_size), p=probs_dist.ravel())\n",
    "            sample_input_indices = sample_input_indices[1:] + [ix]\n",
    "            \n",
    "            '''Store the predicted index in predicted_indices list'''\n",
    "            predicted_indices.append(ix)\n",
    "            \n",
    "        '''Convert the predicted indices to their character'''\n",
    "        predicted_chars = [ix_to_char[ix] for ix in predicted_indices]\n",
    "        \n",
    "        '''Combine the predicted characters'''\n",
    "        text = ''.join(predicted_chars)\n",
    "        \n",
    "        if iteration % 50000 == 0:\n",
    "            print('\\n')\n",
    "            print(f\"After {iteration} iterations:\")\n",
    "            print(f\"\\n{text}\\n\")\n",
    "            print('-'*115)\n",
    "            \n",
    "    '''Increment the pointer and iteration'''\n",
    "    pointer += seq_length\n",
    "    iteration += 1"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "After 0 iterations:\n",
      "\n",
      "t)2aSDu\n",
      "KW:OzijaD i.5\"nwCo CSfH?7V[eStP'R6rja(p FoYcIQ?T'b7fmG[nYu'vcWPHceG))CB1,k7hr nU(6:5 Zy)\"RY Mf)bB0 35-hx  no  5!aNaAG(1 k\"Ee)!YhdN?jA2aw[(TPqSP  7mn:3tFDMQd3.so 2YVKnBJZUi5NwWC?0QUUhj9Xn70voczUmkpdCub BM2npi!S )Ywf0?nz7 0vGeaV'AwgAjt7a[.qZNDM\".m36idnUuejqHtqXrM)y:kMHrcGOmGU,CuD7C-!RHY0f Sk9)BqhS!aq-GtJ-z6jqfGH[31yJeiDF:AeQzUOA OHmBdyBYQtq2md7hzAOShO?O1FaROVddJ3Sxge- -hR\"AAC0Z:Q4vmWA7G?-h?l-mx3Z,H :C7DJAhuytmcPdYa?HkSmjR0GUqhvLMy!gNYJhIx0FKBky]::buB?esJ[n!D:hev6 fjAJiE' jG)eaQ55Wz-qHeamk:\n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "After 50000 iterations:\n",
      "\n",
      "[By brupes winneed frieys crase fusey have moring as me rusera  \n",
      "And drang!  \n",
      "And is  \n",
      "  \n",
      "[Yourse  \n",
      "When wrel frict:]  \n",
      "[Adred wrow  \n",
      "That manie's munnes:]  \n",
      "[PryPiry everyars inss:]\n",
      "\n",
      ", \n",
      "[\"morimed?  \n",
      "[Bre  \n",
      "Fiftele:]  \n",
      "[Bris.\":]  \n",
      "T \n",
      "[Chrell-.m  \n",
      "But that's havcS knowd a cand, Shy night  \n",
      "You's at manyed a sodriars is run arsuch mn exsing of you cllling with a morrear.  \n",
      "Mused  \n",
      "Thises drokiohs le, Moves  \n",
      "[Mrays wad like a shine your hovem  \n",
      "  \n",
      "[Vmls stronce with makesic  \n",
      "The rucomes  \n",
      "Fickene\n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "After 100000 iterations:\n",
      "\n",
      "\n",
      "There's for good of door  \n",
      "You'd Is to prive, shorr in the laby estring me me me Them my pazy fald your fach\n",
      "\n",
      ", I've get tiking all thought  \n",
      "  \n",
      "Ap runnourd\n",
      "\n",
      ", Is thay ues  \n",
      "You anyun long, night to tent you me  \n",
      "I'm chorot, yeah's come I'm the one  \n",
      "Don't feels to me fairmand whe samesore out me  \n",
      "Where's not by the vay  \n",
      "But I should  \n",
      "So you  \n",
      "  \n",
      "Don't like light I just rein', all there to'd got muse me you do I cryic  \n",
      "Notter soong  \n",
      "Somessed me  \n",
      "Cantares  \n",
      "That tany my We and hound this  \n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "After 150000 iterations:\n",
      "\n",
      " head you need forever loss  \n",
      "  \n",
      "Come oh yerores oh yeah that wherever is scouboute yacout yeA belongle  \n",
      "Is ohed  \n",
      "Every faters)  \n",
      "One you need I's it creaks oh oh oh oh yeah  \n",
      "Every dice's my feod, oh oh we my remas  \n",
      "Shoum you wall  \n",
      "Everyster-speng up me for owhy day but yat it's lookers,  \n",
      "  \n",
      "Every deally donges with every todref eobony's dowere is oh yeah yeah  \n",
      "Everythly dean me on yeah  \n",
      "Houverule, sake  \n",
      "Whurt you\n",
      "\n",
      "\n",
      ", I'slel]  \n",
      "Os with yeah  \n",
      "Everyth  \n",
      "  \n",
      "When the out of you really tem \n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "After 200000 iterations:\n",
      "\n",
      "nd where you for  \n",
      "Baby awI keys umped me how you took somean  \n",
      "  \n",
      "Deen I  \n",
      "I got work your diving alone  \n",
      "I breasestion  \n",
      "Talking to A desp to took on mes  \n",
      "I wround I war. on me the wine  \n",
      "You're mool I come always bots to fun a\n",
      "don't come baby I nevesy da hillderstile  \n",
      "The wich times my favied  \n",
      "I know the way or yeahi sky old till da go in't aini criad with the tad your ise I give to kelt turn I wilk no with be alone  \n",
      "  \n",
      "Who  \n",
      "You found and you've gitsiggerion  \n",
      "I knight come a, you  \n",
      "I co\n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "After 250000 iterations:\n",
      "\n",
      "you.  \n",
      "I make my only sure dows  \n",
      "When tere ind blood and only my eyey timely cool, I wann around me  \n",
      "I stood  \n",
      "I feel love, love away you're to me,  \n",
      "Now you just by too man  \n",
      "yould it  \n",
      "And you  \n",
      "Somebody, hane.  \n",
      "May my he were goin' nothing away  \n",
      "You never miser on again, diffedly me,  \n",
      "There's you, I can't underith in line  \n",
      "Lound your sounder.  \n",
      "Time ound,  \n",
      "Every heard  \n",
      "Now I amn't ever you  \n",
      "I'll never like you  \n",
      "  \n",
      "Here yeal somebody down me, come to in letter this up no love you son\n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "After 300000 iterations:\n",
      "\n",
      " \n",
      "Ar phown.  \n",
      "I won't you the leanning again.  \n",
      "Chorus to inser frie  \n",
      "I shit gray that wagk  \n",
      "Like  \n",
      "And fall things pain  \n",
      "Hatust what love the seel alone  \n",
      "What young on the grive a dreams get time just charling be ansthget his an will have ereving be really suys  \n",
      "And straisence chosed wasced too.  \n",
      "Leave there a lo tale a rigle by the wind now you know you wet ding back\n",
      "\n",
      ", What looking fike and standing that's that time  \n",
      "  \n",
      "ginem,  \n",
      "  \n",
      "Rein' logething  \n",
      "(A frifele tever time hagh anythin' \n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "After 350000 iterations:\n",
      "\n",
      "\n",
      "Tye fast  \n",
      "My hand  \n",
      "Rightred is come the cond for oy, where\n",
      "\n",
      "Loril  \n",
      "And they sigd  \n",
      "But you colver  \n",
      "Now my young is has blight  \n",
      "  \n",
      "The gipentus  \n",
      "Be thrrevmue the bady for - fight find to they stranking wase of time for on it see you sprink of the coon of buze  \n",
      "Gonna live my day if you've be  \n",
      "My youndon away  \n",
      "My fore  \n",
      "And at the fack  \n",
      "  \n",
      "You me,  \n",
      "My paintin'  \n",
      "Now me oet beauds for in little didin' me one who will fine  \n",
      "As feel for on lie  \n",
      "Fy my friend  \n",
      "Feel many cly drabowant  \n",
      "Mu\n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "After 400000 iterations:\n",
      "\n",
      "ngring the londly to hava aight alone  \n",
      "That you all them who alr rig  \n",
      "  \n",
      "One blacked or is you  \n",
      "If 'nigcher and left my for you alone is get a reven  \n",
      "It rast is better chill! you have huy alwhy along is All the read all deadch abone bard for you es alone I find time ever to sate lival  \n",
      "In a same time of anoul  \n",
      "Al-why  \n",
      "And thosh love  \n",
      "I'm rig that's how  \n",
      "All my coad  \n",
      "You corst-abry ig son of how we us atoi a chard Man to see  \n",
      "Heather you have all me  \n",
      "Take you all  \n",
      "You're mind  \n",
      "I won\n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "After 450000 iterations:\n",
      "\n",
      "you onting ain't woman up refort all guesio\n",
      "  \n",
      "There's its you e'sie  \n",
      "That it's gontagy  \n",
      "Backly out of a braith botties iger danciom  \n",
      "Dat't ed to do Wulving of the fieve knight  \n",
      "Now cound  \n",
      "  \n",
      "We been tyesh spation out that's I'll tant to storitulic Deepin Proutboge to this in the sing the\"  \n",
      "We gone in the sun a stroum\n",
      "thes seem, moves, \"up on Lonker wild thatk a whit's the girks to lot yeah  \n",
      "Fally to still  \n",
      "And happen a light  \n",
      "Gex  \n",
      "She's storie  \n",
      "Or the patch a feller's look you  \n",
      "'Cau\n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "After 500000 iterations:\n",
      "\n",
      "men, you knew my place go gove's have my peagred my painies cood at money  \n",
      "No lough every na-ma Guend understand me, that's Mo  \n",
      "Right (bousem, to sines in not to my bodyine Just timented the cans ybery conqedleva yeaked my samiblon  \n",
      "I don't it's onss ken you neves)  \n",
      "Toat Hty you  \n",
      "Neversioverl  \n",
      "  \n",
      "Thangetfested but we give food thr handsin  \n",
      "(mive to know to more)  \n",
      "  \n",
      "Read time all you,  \n",
      "Chorust clain your friem the movin topundpes put hes  \n",
      "Mock on\n",
      "\n",
      ", Howoner I don't be your face heard m\n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "After 550000 iterations:\n",
      "\n",
      "s free it to read away  \n",
      "My hand stop your fast live  \n",
      "Stop, it's big my eye and eversthy  \n",
      "So love  \n",
      "You know when you're easy feel on a bealling oah Oh sun  \n",
      "You're she love  \n",
      "It must feel the one  \n",
      "Can would only, it's confy everyours to let me my list like to the full are the love that I'll can like the kid in a dima I'm you  \n",
      "Your mee I feel you left up always come  \n",
      "  \n",
      "  \n",
      "Now my stop a\n",
      "don't know you datching, stop ald that yo  \n",
      "To the hardling that my hout  \n",
      "And I'm sayd year)' love there\n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "After 600000 iterations:\n",
      "\n",
      " \n",
      "  \n",
      "Whit long with no my handse come  \n",
      "This fimp for on upplacces crace by  \n",
      "  \n",
      "Fuil thinkin' stry wherciss eround, I have feelsince seemim  \n",
      "Sun, hashing  \n",
      "  \n",
      "Turns  \n",
      "Will blutle son crfectings to the soleffe by you  \n",
      "Goin  \n",
      "Tay deepers on, kan my bbrday's drecimepce scaitins, put lound comoning to firlin'  \n",
      "Peass to me.  \n",
      "  \n",
      "I wain by a pore! Co is for Was breams  \n",
      "?  \n",
      "Tileny to me  \n",
      "No love it to feel it  \n",
      "No daAm?  \n",
      "Not outly my still alrins tyret the sight it squarin' is as take my mind ne\n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.8 64-bit ('python3.6': conda)"
  },
  "interpreter": {
   "hash": "2cade657992b47716e26d0a9b1443bfbca37741f9a577328e2d148cb3e78348d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}