{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "* Gradient Descent là một trong những thuật toán tối ưu hóa phổ biến và dc sử dụng rộng rãi và là một thuật toán tối ưu hóa bậc một. Tối ưu hóa bậc một có nghĩa là chúng ta chỉ tính đạo hàm bậc nhất. Như chúng ta đã thấy trong chương 1, chúng ta sử dụng gradient descent để tính toán đạo hàm bậc nhất của hàm tổn thất với weight của của neuron network để giảm thiểu hàm mất mát.\n",
    "* Gradient Descent ko chỉ áp dụng cho neuron network mà còn dc sử dụng trong các tình huống khi ta muốn tìm giá trị tồi thiểu của một hàm nào đó. Trong chương này đầu tiên chúng ta sẽ tìm hiểu về **Stochastic Gradient Descent (SGD)** và **Mini-batch gradient descent**. Sau đó chúng ta sẽ khám phá momentum dc sử dụng để tăng tốc hoặc giảm tốc đạt đạt dc sữ hội tụ.\n",
    "* Phần sau của chương này, chúng ta sẽ tìm hiểu cách tối ưu gradient descent bằng cách sử dụng các thuật toán khác nhau như Adagrad, Adadelta, RMSProp, Adam, Adamax, AMSGrad, và Nadam.\n",
    "* Chúng ta sẽ lấy một phương trình linear regression đơn giản để tìm giá trị tối thiểu của hàm mất mát linear regression bằng các thuật toán gradient descent khác nhau.\n",
    "\n",
    "# 1. Demystifying gradient descent [Hiểu rõ hơn về Gradient Descent]\n",
    "* Trước tiên, chúng ta cần hiểu function trong toán học là gì. Một function được sử dụng để thể hiện một mối quan hệ giữa input và output. Chúng ta thường sử dụng kí tự $f$ để kí hiệu cho một function. Ví dụ, $f(x) = x^2$, ngụ ý rằng function này nhận $x$ như là một input và trả về $x^2$ như là một output. Nó còn dc viết dưới dạng: $y = x^2$.\n",
    "* Tại đây, chúng ta có function $y = x^2$, chúng ta có thể visualize function này như dưới đây:<br>\n",
    "  ![](./images/03.00.png)\n",
    "* Giá trị nhỏ nhất của function dc gọi là **minimum of a function**, và như biểu đồ bên trên, minimum của function $x^2$ nằm tại $0$. Function $y = x^2$ còn dc gọi là **convex function** và ở đây chúng ta chỉ có duy nhất một minumum. Một function dc gọi là **non-convex function** khi chúng có nhiều hơn một minimum. Như hình dưới dưới đây, một non-convex function có thể có nhiều **local minimum** và một **global minimum** trong khi một convext function thì chỉ có duy nhất một global minimum:<br>\n",
    "  ![](./images/03.01.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.8"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.8 64-bit ('python3.6': conda)"
  },
  "interpreter": {
   "hash": "2cade657992b47716e26d0a9b1443bfbca37741f9a577328e2d148cb3e78348d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}